\documentclass[10pt]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr,url,hyperref}
\usepackage{graphicx,xspace}
\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,positioning,fit,through}

\oddsidemargin 0in  %0.5in
\topmargin     0in
\leftmargin    0in
\rightmargin   0in
\textheight    9in
\textwidth     6in %6in
%\headheight    0in
%\headsep       0in
%\footskip      0.5in

\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem{conjecture}{Conjecture}

\pagestyle{fancy}

\lhead{\textsc{Prof. Baumer}}
\chead{\textsc{MTH 220: Lecture notes}}
\lfoot{}
\cfoot{}
%\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.0pt}

\newcommand{\ans}{\vspace{0.25in}}
\newcommand{\R}{{\sf R}\xspace}
\newcommand{\cmd}[1]{\texttt{#1}}
\newcommand{\Ex}{\mathbb{E}}

\rhead{\textsc{September 26th, 2016}}

\begin{document}

\paragraph{Agenda}
\begin{enumerate}
  \itemsep0em
  \item Leverage, influence, and outliers
  \item Parallel Slopes Models
\end{enumerate}



\paragraph{Warmup: Regression}

\begin{enumerate}

  \item In 1966 \href{http://en.wikipedia.org/wiki/Cyril_Burt}{Cyril Burt} published a paper called ``The genetic determination of differences in intelligence: A study of monozygotic twins reared apart." The data consist of IQ scores for [an assumed random sample of] 27 identical twins, one raised by foster parents, the other by the biological parents. 

Here is the regression output for using $Biological$ IQ to predict $Foster$ IQ:

<<message=FALSE, size='footnotesize', eval=TRUE>>=
require(mosaic)
require(faraway)
mod <- lm(Foster ~ Biological, data = twins)
coef(mod)
rsquared(mod)
@

Which of the following is \textbf{FALSE}? Justify your answers.

\begin{enumerate}
  \itemsep0.7in
\item Alice and Beth were raised by their biological parents. If Beth's IQ is 10 points higher than Alice's, then we would expect that her foster twin Bernice's IQ is 9 points higher than the IQ of Alice's foster twin Ashley. 
\item Roughly 78\% of the foster twins' IQs can be accurately predicted by the model.
\item The linear model is $\widehat{Foster} = 9.2 + 0.9 \times Biological$.
\item Foster twins with IQs higher than average are expected to have biological twins with higher than average IQs as well.
  \vspace{0.5in}
\end{enumerate}



  \item The scatterplot and least squares summary below show the relationship between weight measured in kilograms and height measured in centimeters of 507 physically active individuals.
  <<message=FALSE, fig.width=10, fig.height=4, eval=TRUE, size='footnotesize'>>=
require(openintro)
qplot(data = bdims, x = hgt, y = wgt, xlab = "Height (cm)", ylab = "Weight (kg)")
coef(lm(wgt ~ hgt, data = bdims))
@
 
\begin{enumerate}
  \itemsep0.7in
\item Describe the relationship between height and weight.
\item Write the equation of the regression line. Interpret the slope and intercept in context.
\item The correlation coefficient for height and weight is 0.72. Calculate $R^2$ and interpret it in context.
  \vspace{0.5in}
\end{enumerate}

\end{enumerate}


\paragraph{Outliers, Leverage, and Influence}

It is important to identify the outliers and understand their role in determing the regression line.

\begin{itemize}
  \itemsep0in
  \item An \emph{outlier} is an observation that doesn't seem to fit the general pattern of the data
  \item An observation with an extreme value of the explanatory variable is a point of high \emph{leverage}
  \item A high leverage point that exerts disproportionate influence on the slope of the regression line is an \emph{influential point}
\end{itemize}

<<message=FALSE, fig.height=4, echo=FALSE, eval=FALSE>>=
ds = reshape(anscombe, direction="long", varying=matrix(names(anscombe), ncol=4, byrow=TRUE), v.names=c("x", "y"))
ds.out = filter(ds, time == 1)
xyplot(y ~ x, data=ds.out, type=c("p", "r"), pch=19, cex=2, lwd = 5, ylim=c(5,13))
ds.out[3, "y"] <- 9
xyplot(y ~ x, data=ds.out, type=c("p", "r"), pch=19, cex=2, lwd = 5, ylim=c(5,13))
ds.out[4, "y"] <- 12
xyplot(y ~ x, data=ds.out, type=c("p", "r"), pch=19, cex=2, lwd = 5, ylim=c(5,13))
@

\paragraph{Quick True or False}

  \begin{enumerate}
\item Influential points always change the intercept of the regression line.
\item Influential points always reduce $R^2$.
\item It is much more likely for a low leverage point to be influential, than a high leverage point.
  \end{enumerate}

\newpage

\paragraph{Multiple Regression}

Multiple regression is a natural extension of simple linear regression. 
\begin{itemize}
  \itemsep0in
  \item SLR: one response variable, one explanatory variable
  $$
    Y = \beta_0 + \beta_1 \cdot X + \epsilon, \text{ where } \epsilon \sim N(0, \sigma_\epsilon)
  $$
  \item MLR: one response variable, \emph{more than one} explanatory variable 
  $$
    Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \cdots + \beta_p \cdot X_p + \epsilon, \text{ where } \epsilon \sim N(0, \sigma_\epsilon)
  $$
  \item Estimated coefficients (e.g. $\hat{\beta}_i$'s) now are interpreted in relation to (or ``conditional on'') the other variables
  \item $\beta_i$ reflects the \emph{predicted} change in $Y$ associated with a one unit increase in $X_i$, conditional upon the rest of the $X_i$'s. 
  \item $R^2$ has the same interpretation (proportion of variability explained by the model)
\end{itemize}


\paragraph{Multiple Regression with a Categorical Variable}

Consider the case where $X_1$ is quantitative, but $X_2$ is an \emph{indicator} variable that can only be 0 or 1 (e.g. $isFemale$). Then,
$$
  \hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 \cdot X_1 + \hat{\beta}_2 \cdot X_2
$$
So then,
  \begin{align*}
    \text{For men, } \qquad \hat{Y} |_{ X_1, X_2 = 0} &= \hat{\beta}_0 + \hat{\beta}_1 \cdot X_1 \\ 
    \text{For women, } \qquad \hat{Y} |_{ X_1, X_2 = 1} &= \hat{\beta}_0 + \hat{\beta}_1 \cdot X_1 + \hat{\beta}_2 \cdot 1 \\
      &= \left( \hat{\beta}_0 + \hat{\beta}_2 \right) + \hat{\beta}_1 \cdot X_1  
  \end{align*}
This is called a \emph{parallel slopes} model. [Why?]
  
\paragraph{Example: Italian Restaurants}

The Zagat guide contains restaurant ratings and reviews for many major world cities. We want to understand variation in the average $Price$ of a dinner in Italian restaurants in New York City. Specifically, we want to know how customer ratings (measured on a scale of 0 to 30) of the $Food$, $Decor$, and $Service$, as well as whether the restaurant is located to the $East$ or west of 5th Avenue, are associated with the average $Price$ of a meal. The data contains ratings and prices for 168 Italian restaurants in 2001.

<<message=FALSE,fig.show='hold', fig.height=4>>=
NYC <- read.csv("http://www.math.smith.edu/~bbaumer/mth241/nyc.csv")
ggplot(data = NYC, aes(x = jitter(Service), y = Price)) +
  geom_point(alpha = 0.5, size = 2) + geom_smooth(method = "lm", se = 0) +
  xlab("Jittered service rating") + ylab("Average Price (US$)")
lm(Price ~ Service, data = NYC)
@

\paragraph{In-Class Activity}

\begin{enumerate}
  \itemsep1.2in
  \item Use {\tt qplot()} to examine the bivariate relationships between $Price$, $Food$ and $Service$. 
  \item What do you observe? Describe the form, direction, and strength of the relationships.
  \item Use {\tt lm()} to build a SLR model for $Price$ as a function of $Food$. Interpret the coefficients of this model. How is the quality of the food at these restaurants associated with its price? 
  \item Build a parallel slopes model by conditioning on the $East$ variable. Interpret the coefficients of this model. What is the value of being on the East Side of Fifth Avenue?
  \vspace{1in}
\end{enumerate}


% \newpage
% 
% \paragraph{Instructor's Notes}

<<include=FALSE>>=
require(mosaic)
x = rnorm(100)
y = rnorm(100)
ds = data.frame(y,x)
xyplot(y ~ x, data=ds, type=c("p", "r"), lwd=3)
rsquared(lm(y ~ x, data=ds))
@


<<include=FALSE>>=
ds = rbind(ds, data.frame(y=2, x=10))
xyplot(y ~ x, data=ds, type=c("p", "r"), lwd=3)
rsquared(lm(y ~ x, data=ds))
@




\end{document}
